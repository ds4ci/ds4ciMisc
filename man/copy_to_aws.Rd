% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/copy_to_aws.R
\name{copy_to_aws}
\alias{copy_to_aws}
\title{Copy a Local Data Frame to a DBI Backend on AWS.}
\usage{
copy_to_aws(con, df, schema = "public", tname = deparse(substitute(df)),
  s3_bucket, s3_folder = "ds4ci_temp", overwrite = FALSE, clear_s3 = TRUE,
  temporary = FALSE, identity_column = FALSE, types = NULL, ...)
}
\arguments{
\item{con}{A DBI, or odbc, connection to the AWS Redshift database to copy to}

\item{df}{The data.frame or tbl_df to copy}

\item{schema}{The schema in the datbase}

\item{tname}{The table name in the schema}

\item{s3_bucket}{The S3 bucket to use as a "buffer"}

\item{s3_folder}{A folder in the bucket. Will be created if it doesn't exist already}

\item{overwrite}{TRUE to truncate the table before \code{COPY}ing}

\item{clear_s3}{TRUE deletes the temp s3 file before returning}

\item{temporary}{Placeholder - not used in initial release}

\item{identity_column}{Placeholder - not used in initial release}

\item{types}{Placeholder - not used in initial release}
}
\value{
if the \code{COPY} was successful, the number of rows copied. If unsuccessful, the Redshift error.
}
\description{
This \code{copy_to_aws()} method assumes an AWS Redshift database with is accessable from S3. Unlike
\code{dbplyr::copy_to()}, on which it is loosly modeled, it is optimized for large dataframes. It is
essentially a bulk loader of R dataframes, or tbls, to a Redshift table - which may, or may not, already
exist. \strong{For initial release table must exist!}
}
\details{
\code{copy_to_aws()} leverages the Redshift \code{COPY} command of a pipe-delimited gzip'd file of the
dataframe which it uploads to a specified S3 bucket and folder. The file is deleted if no error is thrown.

\strong{Assumptions:}

\itemize{
  \item INITIAL RELEASE ASSUMES TARGET schema.table EXISTS
  \item The S3 credentials have been set up as described in \url{https://github.com/cloudyr/aws.s3}
}
}
\examples{

}
